{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "from model_architectures import Encoder_RNN, Decoder_RNN\n",
    "from data_prep import prepareTrainData, tensorsFromPair, prepareNonTrainDataForLanguagePair, load_cpickle_gc\n",
    "from inference import generate_translation\n",
    "from misc import timeSince, load_cpickle_gc\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PAD_token = 0\n",
    "PAD_TOKEN = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "teacher_forcing_ratio = 1.0\n",
    "attn_model = 'dot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguagePairDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sent_pairs): \n",
    "        # this is a list of sentences \n",
    "        self.sent_pairs_list = sent_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_pairs_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1 = self.sent_pairs_list[key][0]\n",
    "        sent2 = self.sent_pairs_list[key][1]\n",
    "        return [sent1, sent2, len(sent1), len(sent2)]\n",
    "\n",
    "def language_pair_dataset_collate_function(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_list = []\n",
    "    sent2_length_list = []\n",
    "    # padding\n",
    "    # NOW PAD WITH THE MAXIMUM LENGTH OF THE FIRST and second batches \n",
    "    max_length_1 = max([len(x[0]) for x in batch])\n",
    "    max_length_2 = max([len(x[1]) for x in batch])\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]).T.squeeze(), pad_width=((0,max_length_1-len(datum[0]))), \n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        padded_vec_2 = np.pad(np.array(datum[1]).T.squeeze(), pad_width=((0,max_length_2-len(datum[1]))), \n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        sent1_list.append(padded_vec_1)\n",
    "        sent2_list.append(padded_vec_2)\n",
    "        sent1_length_list.append(len(datum[0]))\n",
    "        sent2_length_list.append(len(datum[1]))\n",
    "    return [torch.from_numpy(np.array(sent1_list)), torch.cuda.LongTensor(sent1_length_list), \n",
    "            torch.from_numpy(np.array(sent2_list)), torch.cuda.LongTensor(sent2_length_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Counted words:\n",
      "vi 14362\n",
      "en 41272\n",
      "Reading lines...\n",
      "['khi toi con nho toi nghi rang bactrieu tien la at nuoc tot nhat tren the gioi va toi thuong hat bai chung ta chang co gi phai ghen ti . ', 'when i was little i thought my country was the best on the planet and i grew up singing a song called nothing to envy . ']\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "['khi toi con nho toi nghi rang bactrieu tien la at nuoc tot nhat tren the gioi va toi thuong hat bai chung ta chang co gi phai ghen ti . ', 'when i was little i thought my country was the best on the planet and i grew up singing a song called nothing to envy . ']\n",
      "Counted words:\n",
      "vi 1107\n",
      "en 3570\n"
     ]
    }
   ],
   "source": [
    "input_lang, target_lang, train_pairs = prepareTrainData(\n",
    "    \"iwslt-vi-en-processed/train.vi\",\n",
    "    \"iwslt-vi-en-processed/train.tok.en\",\n",
    "    input_lang = 'vi',\n",
    "    target_lang = 'en')\n",
    "\n",
    "train_idx_pairs = []\n",
    "for x in train_pairs:\n",
    "    indexed = list(tensorsFromPair(x, input_lang, target_lang))\n",
    "    train_idx_pairs.append(indexed)\n",
    "\n",
    "pickle.dump(input_lang, open(\"input_lang_vi\", \"wb\"))\n",
    "pickle.dump(target_lang, open(\"target_lang_en\", \"wb\"))\n",
    "pickle.dump(train_idx_pairs, open(\"train_vi_en_idx_pairs\", \"wb\"))\n",
    "\n",
    "_, _, val_pairs = prepareTrainData(\n",
    "    \"iwslt-vi-en-processed/dev.vi\",\n",
    "    \"iwslt-vi-en-processed/dev.en\",\n",
    "    input_lang = 'vi',\n",
    "    target_lang = 'en')\n",
    "\n",
    "val_idx_pairs = []\n",
    "for x in val_pairs:\n",
    "    indexed = list(tensorsFromPair(x, input_lang, target_lang))\n",
    "    val_idx_pairs.append(indexed)\n",
    "\n",
    "pickle.dump(val_pairs, open(\"val_pairs\", \"wb\"))\n",
    "pickle.dump(val_idx_pairs, open(\"val_idx_pairs\", \"wb\"))\n",
    "\n",
    "\n",
    "input_lang = load_cpickle_gc(\"input_lang_vi\")\n",
    "target_lang = load_cpickle_gc(\"target_lang_en\")\n",
    "train_idx_pairs = load_cpickle_gc(\"train_vi_en_idx_pairs\")\n",
    "val_idx_pairs = load_cpickle_gc(\"val_idx_pairs\")\n",
    "val_pairs = load_cpickle_gc(\"val_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LanguagePairDataset(train_idx_pairs)\n",
    "# is there anything in the train_idx_pairs that is only 0s right noww instea dof padding. \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=language_pair_dataset_collate_function,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check SOS Token: Do both train and val start with that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Batch_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder_Batch_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        '''\n",
    "            sents is (batch_size by padded_length)\n",
    "            when we evaluate sentence by sentence, you evaluate it with batch_size = 1, padded_length.\n",
    "            [[1, 2, 3, 4]] etc. \n",
    "        '''\n",
    "        batch_size = sents.size()[0]\n",
    "        sent_lengths = list(sent_lengths)\n",
    "        # We sort and then do pad packed sequence here. \n",
    "        descending_lengths = [x for x, _ in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_indices = [x for _, x in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_lengths = torch.tensor(descending_lengths)\n",
    "        descending_sents = torch.index_select(sents, 0, torch.tensor(descending_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(descending_sents)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        rnn_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # rnn_out is 32 by 72 by 256\n",
    "        \n",
    "        # change the order back\n",
    "        change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device))  \n",
    "        rnn_out = torch.index_select(rnn_out, 0, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "        return rnn_out, self.hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        # hidden is 32 by 256\n",
    "        # encoder_outputs is 32 by 72 by 256\n",
    "        hidden = hidden[0]\n",
    "        batch_size = hidden.size()[0]\n",
    "        attn_energies = []\n",
    "        for i in range(batch_size):\n",
    "            attn_energies.append(self.score(hidden[i], encoder_outputs[i]))\n",
    "        \n",
    "        # attn_energies is 32 by 72\n",
    "        attn_energies = self.softmax(torch.stack(attn_energies))\n",
    "        \n",
    "        context_vectors = []\n",
    "        for i in range(batch_size):\n",
    "            context_vectors.append(torch.matmul(attn_energies[i], encoder_outputs[i]))\n",
    "                \n",
    "        context_vectors = torch.stack(context_vectors)\n",
    "        \n",
    "        return context_vectors\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':            \n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 22 by 256\n",
    "            encoder_output = torch.transpose(encoder_output, 0, 1)\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, self.attn(encoder_output))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            len_encoder_output = encoder_output.size()[1]\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            hidden = torch.transpose(hidden, 0, 1)\n",
    "            # hidden is 256 by 1\n",
    "            hidden = hidden.repeat(hidden_size, len_encoder_output)\n",
    "            # hidden is 256 by 22\n",
    "            concat = torch.cat((hidden, encoder_output), dim=0)\n",
    "            # concat is 512 by 22\n",
    "            # self.attn(concat) --> 256 by 22\n",
    "            energy = torch.matmul(self.v, F.tanh(self.attn(concat)))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = self.attn(rnn_output, encoder_outputs)\n",
    "        # context is 32 by 256\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        # rnn_output is 32 by 256\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        # output is 32 by vocab_size\n",
    "        output = self.LogSoftmax(output)\n",
    "\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, pairs, validation_pairs, lang1, lang2, search, title, max_length_generation,  print_every=1000, plot_every=1000, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    Max length generation is the max length generation you want \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    plot_losses, val_losses = [], []\n",
    "    val_losses = [] \n",
    "    count, print_loss_total, plot_loss_total, val_loss_total, plot_val_loss = 0, 0, 0, 0, 0 \n",
    "    encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate)\n",
    "    #encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode=\"min\")\n",
    "    #decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode=\"min\")\n",
    "\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token) # this ignores the padded token. \n",
    "    plot_loss =[]\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        plot_loss = []\n",
    "        val_loss = []\n",
    "        for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            encoder_outputs, encoder_hidden = encoder(sent1_batch, sent1_length_batch)\n",
    "            # outputs is 32 by 72 by 256\n",
    "            # encoder_hidden is 1 by 32 by 256\n",
    "            \n",
    "            decoder_input = torch.LongTensor([SOS_token] * BATCH_SIZE).view(-1, 1).to(device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # decoder_input is 32 by 1\n",
    "            # decoder_hidden is 1 by 32 by 256\n",
    "                        \n",
    "            max_trg_len = max(sent2_lengths)\n",
    "            loss = 0\n",
    "            \n",
    "            # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "            for t in range(max_trg_len):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                # decoder_output is 32 by vocab_size\n",
    "                # sent2_batch is 32 by 46\n",
    "                loss += criterion(decoder_output, sent2_batch[:, t])\n",
    "                decoder_input = sent2_batch[:, t]\n",
    "             \n",
    "            loss = loss / max_trg_len.float()\n",
    "            print_loss_total += loss\n",
    "            count += 1\n",
    "            loss.backward()\n",
    "        \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if (step+1) % print_every == 0:\n",
    "                # lets train and plot at the same time. \n",
    "                print_loss_avg = print_loss_total / count\n",
    "                count = 0\n",
    "                print_loss_total = 0\n",
    "                print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, step / n_epochs),\n",
    "                                             step, step / n_epochs * 100, print_loss_avg))\n",
    "                # 42s\n",
    "#                 v_loss = test_model(encoder, decoder, search, validation_pairs, lang2, max_length=max_length_generation)\n",
    "                # returns bleu score\n",
    "#                 print(\"VALIDATION BLEU SCORE: \"+str(v_loss))\n",
    "#                 val_loss.append(v_loss)\n",
    "                plot_loss.append(print_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    save_model(encoder, decoder, val_losses, plot_losses, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "TRAIN SCORE 0m 6s (- 0m 0s) (9 90%) 10.5898\n",
      "TRAIN SCORE 0m 11s (- -1m 54s) (19 190%) 10.5421\n",
      "TRAIN SCORE 0m 16s (- -1m 48s) (29 290%) 10.4422\n",
      "TRAIN SCORE 0m 23s (- -1m 42s) (39 390%) 10.3596\n",
      "TRAIN SCORE 0m 32s (- -1m 33s) (49 490%) 10.2316\n",
      "TRAIN SCORE 0m 37s (- -1m 28s) (59 590%) 9.9317\n",
      "TRAIN SCORE 0m 43s (- -1m 23s) (69 690%) 9.6662\n",
      "TRAIN SCORE 0m 49s (- -1m 16s) (79 790%) 9.5689\n",
      "TRAIN SCORE 0m 54s (- -1m 11s) (89 890%) 8.9570\n",
      "TRAIN SCORE 1m 0s (- -1m 5s) (99 990%) 8.5964\n",
      "TRAIN SCORE 1m 6s (- -2m 59s) (109 1090%) 8.1421\n",
      "TRAIN SCORE 1m 13s (- -2m 52s) (119 1190%) 7.9371\n",
      "TRAIN SCORE 1m 20s (- -2m 46s) (129 1290%) 7.7406\n",
      "TRAIN SCORE 1m 25s (- -2m 40s) (139 1390%) 7.5160\n",
      "TRAIN SCORE 1m 31s (- -2m 34s) (149 1490%) 7.4830\n",
      "TRAIN SCORE 1m 36s (- -2m 29s) (159 1590%) 7.3020\n",
      "TRAIN SCORE 1m 41s (- -2m 24s) (169 1689%) 7.2297\n",
      "TRAIN SCORE 1m 47s (- -2m 18s) (179 1789%) 7.3220\n",
      "TRAIN SCORE 1m 52s (- -2m 13s) (189 1889%) 7.0043\n",
      "TRAIN SCORE 1m 57s (- -2m 8s) (199 1989%) 6.9872\n",
      "TRAIN SCORE 2m 4s (- -2m 1s) (209 2090%) 7.1774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-372999c5d6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-629ec6fd3794>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_epochs, pairs, validation_pairs, lang1, lang2, search, title, max_length_generation, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = Encoder_Batch_RNN(input_lang.n_words, hidden_size).to(device)\n",
    "# decoder1 = Decoder_Batch_2RNN(target_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = LuongAttnDecoderRNN(attn_model, hidden_size, target_lang.n_words, 1).to(device)\n",
    "args = {\n",
    "    'n_epochs': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'search': 'beam',\n",
    "    'encoder': encoder1,\n",
    "    'decoder': decoder1,\n",
    "    'lang1': input_lang, \n",
    "    'lang2': target_lang,\n",
    "    \"pairs\":train_idx_pairs, \n",
    "    \"validation_pairs\": val_idx_pairs[:200], \n",
    "    \"title\": \"Training Curve for Basic 1-Directional Encoder Decoder Model With LR = 0.0001\",\n",
    "    \"max_length_generation\": 20, \n",
    "    \"plot_every\": 10, \n",
    "    \"print_every\": 10\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "We follow https://arxiv.org/pdf/1406.1078.pdf \n",
    "and use the Adadelta optimizer\n",
    "\n",
    "\"\"\"\n",
    "print(BATCH_SIZE)\n",
    "\n",
    "trainIters(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
