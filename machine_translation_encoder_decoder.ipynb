{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "from data_prep import prepareData\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Read 50000 sentence pairs\n",
      "Trimmed to 37026 sentence pairs\n",
      "Counting words...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Counted words:\n",
      "vi 5551\n",
      "eng 19344\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "input_file = 'iwslt-vi-en-processed/train.vi'\n",
    "target_file = 'iwslt-vi-en-processed/train.en'\n",
    "input_lang, target_lang, pairs = prepareData(input_file, target_file, 'vi', 'eng', size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs[4][0])\n",
    "print(pairs[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pairs, open(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_indices_pairs_test\", \"wb\"))\n",
    "pickle.dump(input_lang, open(\"preprocessed_data_no_elmo/iwslt-zh-eng/preprocessed_no_elmo_zhlang\", \"wb\"))\n",
    "pickle.dump(target_lang, open(\"preprocessed_data_no_elmo/iwslt-zh-eng/preprocessed_no_elmo_englang\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        # output and hidden are the same vectors\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input).view(1, 1, -1)\n",
    "        embed = F.relu(embed)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(CorrectDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(2*hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_hidden):\n",
    "        embed = self.embedding(input).view(1, 1, -1)\n",
    "        embed = F.relu(embed)\n",
    "        embed_plus_encoded = torch.cat((embed, encoder_hidden), 2)\n",
    "        output, hidden = self.gru(embed_plus_encoded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(target_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def indexesFromSentence(lang, sentence):\n",
    "#     words = sentence.split(' ')\n",
    "#     indices = []\n",
    "#     for word in words:\n",
    "#         if lang.word2index.get(word) is not None:\n",
    "#             indices.append(lang.word2index[word])\n",
    "#         else:\n",
    "#             indices.append(1) # UNK_INDEX\n",
    "#     return indices\n",
    "\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair, input_lang, target_lang):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(target_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "# example of input_tensor: [2, 43, 23, 9, 19, 4]. Indexed on our vocabulary. \n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # iterate GRU over words --> final hidden state is representation of source sentence. \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = decoder.initHidden()\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_cpickle_gc(dirlink):\n",
    "#     # https://stackoverflow.com/questions/26860051/how-to-reduce-the-time-taken-to-load-a-pickle-file-in-python\n",
    "#     output = open(dirlink, 'rb')\n",
    "\n",
    "#     # disable garbage collector\n",
    "#     gc.disable()\n",
    "\n",
    "#     mydict = pickle.load(output)\n",
    "\n",
    "#     # enable garbage collector again\n",
    "#     gc.enable()\n",
    "#     output.close()\n",
    "#     return mydict\n",
    "\n",
    "# def trainIters(encoder, decoder, n_iters,n_epochs,  lang1, lang2,  print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "#     \"\"\"\n",
    "#     lang1 is the Lang object for language 1 \n",
    "#     Lang2 is the Lang object for language 2\n",
    "#     \"\"\"\n",
    "#     pairs = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1.name+\"-\"+lang2.name+\"/preprocessed_no_indices_pairs_train\")\n",
    "#     validation_pairs = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1.name+\"-\"+lang2.name+\"/preprocessed_no_indices_pairs_train\")\n",
    "#     start = time.time()\n",
    "#     plot_losses = []\n",
    "#     print_loss_total = 0  # Reset every print_every\n",
    "#     plot_loss_total = 0  # Reset every plot_every\n",
    "#     encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "#     for i in range(n_epochs):\n",
    "#         training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                           for i in range(n_iters)]\n",
    "#         criterion = nn.NLLLoss()\n",
    "#         # framing it as a categorical loss function. \n",
    "#         for iter in range(1, n_iters + 1):\n",
    "#             training_pair = training_pairs[iter - 1] \n",
    "#             d_input_tensor = training_pair[0]\n",
    "#             d_target_tensor = training_pair[1]\n",
    "#             input_tensor = tensorFromSentence(lang1, d_input_tensor)\n",
    "#             target_tensor = tensorFromSentence(lang2, d_target_tensor)\n",
    "#             loss = train(input_tensor, target_tensor, encoder,\n",
    "#                          decoder, encoder_optimizer, decoder_optimizer, criterion, MAX_LENGTH_VI_EN)\n",
    "#             print_loss_total += loss\n",
    "#             plot_loss_total += loss\n",
    "\n",
    "#             if iter % print_every == 0:\n",
    "#                 print_loss_avg = print_loss_total / print_every\n",
    "#                 print_loss_total = 0\n",
    "#                 print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n",
    "#                                              iter, iter / n_epochs * 100, print_loss_avg))\n",
    "#                 val_loss = test_model(encoder, decoder,search, validation_pairs, lang1, max_length=MAX_LENGTH)\n",
    "#                 # retursn teh bleu score\n",
    "#                 print(\"VALIDATION BLEU SCORE: \"+str(val_loss))\n",
    "\n",
    "#             if iter % plot_every == 0:\n",
    "#                 plot_loss_avg = plot_loss_total / plot_every\n",
    "#                 plot_losses.append(plot_loss_avg)\n",
    "#                 plot_loss_total = 0\n",
    "\n",
    "#     showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 30s (- 50m 4s) (100 1%) 5.7816\n",
      "1m 2s (- 50m 55s) (200 2%) 5.4874\n",
      "1m 30s (- 48m 52s) (300 3%) 4.8540\n",
      "2m 1s (- 48m 41s) (400 4%) 5.1465\n",
      "2m 37s (- 49m 50s) (500 5%) 5.3618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8361f1945171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-29eaa6fdc28a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-47e1859dc8a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = CorrectDecoderRNN(target_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "num_iters = 10000\n",
    "trainIters(encoder1, decoder1, num_iters, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder1.state_dict(), \"encoder1_40000\")\n",
    "# torch.save(decoder1.state_dict(), \"decoder1_40000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "encoder1.load_state_dict(torch.load(\"encoder1_40000\", map_location='cpu'))\n",
    "decoder1 = DecoderRNN(target_lang.n_words, hidden_size).to(device)\n",
    "decoder1.load_state_dict(torch.load(\"decoder1_40000\", map_location='cpu'))\n",
    "\n",
    "evaluateRandomly(encoder1, decoder1, n = 10, strategy='beam', k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10, strategy=\"greedy\", k = None):\n",
    "    \"\"\"\n",
    "    Randomly select a sentence from the input dataset and try to produce its translation.\n",
    "    \"\"\"    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = generate_translation(encoder, decoder, pair[0], search=strategy, k = k)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation(encoder, decoder, sentence, max_length=MAX_LENGTH, search=\"greedy\", k = None):\n",
    "    \"\"\" \n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @returns decoded_words: a list of words in target language\n",
    "    \"\"\"    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        # encode the source sentence\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "\n",
    "        # start decoding\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "        \n",
    "        if search == 'greedy':\n",
    "            decoded_words = greedy_search(decoder, decoder_input, decoder_hidden, max_length)\n",
    "        elif search == 'beam':\n",
    "            if k == None:\n",
    "                k = 2\n",
    "            decoded_words = beam_search(decoder, decoder_input, decoder_hidden, max_length, k)  \n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(decoder, decoder_input, hidden, max_length):\n",
    "    translation = []\n",
    "    for i in range(max_length):\n",
    "        next_word_softmax, hidden = decoder(decoder_input, hidden)\n",
    "        best_idx = torch.max(next_word_softmax, 1)[1].squeeze().item()\n",
    "\n",
    "        # convert idx to word\n",
    "        best_word = target_lang.index2word[best_idx]\n",
    "        translation.append(best_word)\n",
    "        decoder_input = torch.tensor([[best_idx]], device=device)\n",
    "        \n",
    "        if best_word == 'EOS':\n",
    "            break\n",
    "    return translation\n",
    "\n",
    "\n",
    "def beam_search(decoder, decoder_input, hidden, max_length, k):\n",
    "    \n",
    "    candidates = [(decoder_input, 0, hidden)]\n",
    "    potential_candidates = []\n",
    "    completed_translations = []\n",
    "\n",
    "    # put a cap on the length of generated sentences\n",
    "    for m in range(max_length):\n",
    "        for c in candidates:\n",
    "            # unpack the tuple\n",
    "            c_sequence = c[0]\n",
    "            c_score = c[1]\n",
    "            c_hidden = c[2]\n",
    "            # EOS token\n",
    "            if c_sequence[-1] == 1:\n",
    "                completed_translations.append((c_sequence, c_score))\n",
    "                k = k - 1\n",
    "            else:\n",
    "                next_word_probs, hidden = decoder(c_sequence[-1], c_hidden)\n",
    "                # in the worst-case, one sequence will have the highest k probabilities\n",
    "                # so to save computation, only grab the k highest_probability from each candidate sequence\n",
    "                top_probs, top_idx = torch.topk(next_word_probs, k)\n",
    "                for i in range(len(top_probs[0])):\n",
    "                    word = torch.from_numpy(np.array([top_idx[0][i]]).reshape(1, 1)).to(device)\n",
    "                    new_score = c_score + top_probs[0][i]\n",
    "                    potential_candidates.append((torch.cat((c_sequence, word)).to(device), new_score, hidden))\n",
    "\n",
    "        candidates = sorted(potential_candidates, key= lambda x: x[1], reverse=True)[0:k] \n",
    "        potential_candidates = []\n",
    "\n",
    "    completed = completed_translations + candidates\n",
    "    completed = sorted(completed, key= lambda x: x[1], reverse=True)[0] \n",
    "    final_translation = []\n",
    "    for x in completed[0]:\n",
    "        final_translation.append(target_lang.index2word[x.squeeze().item()])\n",
    "    return final_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "for x in a: \n",
    "    x = 4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 1)\n",
    "b = torch.from_numpy(np.array([43]).reshape(1, 1))\n",
    "torch.cat((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, search=\"greedy\", max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # encode the source lanugage\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        # output of this function\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        if search == 'greedy':\n",
    "            decoded_words = greedy_search(decoder, decoder_input, decoder_hidden, max_length)\n",
    "        elif search == 'beam':\n",
    "            decoded_words = beam_search(decoder, decoder_input, decoder_hidden, max_length)  \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "def calculate_bleu(predictions, labels):\n",
    "\t\"\"\"\n",
    "\tOnly pass a list of strings \n",
    "\t\"\"\"\n",
    "\t# tthis is ony with n_gram = 4\n",
    "\n",
    "\tbleu = sacrebleu.raw_corpus_bleu(predictions, [labels], .01).score\n",
    "\treturn bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e = EncoderRNN(5551, 256)\n",
    "model_e.load_state_dict(torch.load(\"encoder1_40000\", map_location='cpu'))\n",
    "model_e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = DecoderRNN(19344, 256)\n",
    "model_d.load_state_dict(torch.load(\"decoder1_40000\", map_location='cpu'))\n",
    "model_d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "def test_model(encoder, decoder,search, test_pairs, lang1,max_length=MAX_LENGTH):\n",
    "    # for test, you only need the lang1 words to be tokenized,\n",
    "    # lang2 words is the true labels\n",
    "    encoder_inputs = [pair[0] for pair in test_pairs]\n",
    "    true_labels = [pair[1] for pair in test_pairs]\n",
    "    translated_predictions = []\n",
    "    for i in range(len(encoder_inputs)): \n",
    "        if i% 100== 0:\n",
    "            print(i)\n",
    "        e_input = encoder_inputs[i]\n",
    "        decoded_words = evaluate(encoder, decoder, e_input, max_length=MAX_LENGTH)\n",
    "        translated_predictions.append(\" \".join(decoded_words))\n",
    "    return calculate_bleu(translated_predictions, true_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "    \n",
    "Yikes, teh decoder isn't' preforming very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = pickle.load(open(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_indices_pairs_test\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang = pickle.load(open(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_elmo_vilang\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model_e, model_d, \"greedy\", test_pairs, input_lang )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
