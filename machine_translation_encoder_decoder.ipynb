{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from model_architectures import Encoder_RNN, Decoder_RNN\n",
    "from data_prep import prepareData, tensorsFromPair\n",
    "from inference import generate_translation\n",
    "from misc import timeSince, load_cpickle_gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 30\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep - train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 98221 sentence pairs\n",
      "Counting words...\n",
      "['khoa hoc ang sau mot tieu e ve khi hau', 'rachel pike the science behind a climate headline']\n",
      "Counted words:\n",
      "vi 10502\n",
      "en 31257\n",
      "Reading lines...\n",
      "['khi toi con nho toi nghi rang bactrieu tien la at nuoc tot nhat tren the gioi va toi thuong hat bai chung ta chang co gi phai ghen ti . ', 'when i was little i thought my country was the best on the planet and i grew up singing a song called nothing to envy . ']\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 879 sentence pairs\n",
      "Counting words...\n",
      "['toi a rat tu hao ve at nuoc toi .', 'and i was very proud .']\n",
      "Counted words:\n",
      "vi 877\n",
      "en 2177\n"
     ]
    }
   ],
   "source": [
    "input_lang, target_lang, train_pairs = prepareData(\n",
    "    input_file = 'iwslt-vi-en-processed/train.vi', \n",
    "    target_file = 'iwslt-vi-en-processed/train.en', \n",
    "    input_lang = 'vi', \n",
    "    target_lang = 'en')\n",
    "\n",
    "pickle.dump(train_pairs, open(\"train_vi_en_pairs\", \"wb\"))\n",
    "pickle.dump(input_lang, open(\"train_vi_lang\", \"wb\"))\n",
    "pickle.dump(target_lang, open(\"train_en_lang\", \"wb\"))\n",
    "\n",
    "_, _, val_pairs = prepareData(\n",
    "    input_file = 'iwslt-vi-en-processed/dev.vi', \n",
    "    target_file = 'iwslt-vi-en-processed/dev.en',\n",
    "    input_lang = 'vi', \n",
    "    target_lang = 'en')\n",
    "\n",
    "pickle.dump(val_pairs, open(\"val_vi_en_pairs\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_pairs = []\n",
    "for x in train_pairs:\n",
    "    indexed = list(tensorsFromPair(x, input_lang, target_lang))\n",
    "    train_idx_pairs.append(indexed)\n",
    "\n",
    "pickle.dump(train_idx_pairs, open(\"train_vi_en_idx_pairs\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LanguagePairDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sent_pairs): \n",
    "        # this is a list of sentences \n",
    "        self.sent_pairs_list = sent_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_pairs_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1 = self.sent_pairs_list[key][0][:MAX_LENGTH]\n",
    "        sent2 = self.sent_pairs_list[key][1][:MAX_LENGTH]\n",
    "        return [sent1, sent2, len(sent1), len(sent2)]\n",
    "\n",
    "def language_pair_dataset_collate_function(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_list = []\n",
    "    sent2_length_list = []\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]).T.squeeze(), pad_width=((0,MAX_LENGTH-len(datum[0]))), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[1]).T.squeeze(), pad_width=((0,MAX_LENGTH-len(datum[1]))), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sent1_list.append(padded_vec_1)\n",
    "        sent2_list.append(padded_vec_2)\n",
    "        sent1_length_list.append(len(datum[0]))\n",
    "        sent2_length_list.append(len(datum[1]))\n",
    "    print(np.array(sent1_list).shape)\n",
    "    return [torch.from_numpy(np.array(sent1_list)), torch.LongTensor(sent1_length_list), \n",
    "            torch.from_numpy(np.array(sent2_list)), torch.LongTensor(sent2_length_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LanguagePairDataset(train_idx_pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=language_pair_dataset_collate_function,\n",
    "                                           #shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 30)\n",
      "0\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i, (sent1_list, sent1_lengths, sent2_list, sent2_lengths) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(sent1_list.size())\n",
    "    print(sent2_list.size())\n",
    "    \n",
    "    print(sent1_lengths.size())\n",
    "    print(sent2_lengths.size())\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Batch_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder_Batch_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        '''\n",
    "            sents is a tensor with the shape (batch_size, padded_length )\n",
    "        '''\n",
    "        batch_size = sents.size()[0]\n",
    "        sent_lengths = list(sent_lengths)\n",
    "        \n",
    "        descending_lengths = [x for x, _ in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_indices = [x for _, x in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_lengths = np.array(descending_lengths)\n",
    "        \n",
    "        descending_sents = torch.index_select(sents, 0, torch.tensor(descending_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(descending_sents)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        rnn_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        \n",
    "        # change the order back\n",
    "        change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 0, torch.LongTensor(change_it_back).to(device)) \n",
    "        rnn_out = torch.index_select(rnn_out, 0, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "        # **TODO**: What is rnn_out?\n",
    "        return rnn_out, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  3  6 10 23]\n",
      " [ 5 20  1 89  5]]\n",
      "tensor([[ 4,  3,  6, 10, 23],\n",
      "        [ 5, 20,  1, 89,  5]])\n"
     ]
    }
   ],
   "source": [
    "sent1 = np.array([4, 3, 6, 10, 23])\n",
    "sent2 = np.array([5, 20, 1, 89, 5])\n",
    "sents = np.vstack((sent1, sent2))\n",
    "print(sents)\n",
    "original_sents = torch.from_numpy(sents)\n",
    "print(original_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # example of input_tensor: [2, 43, 23, 9, 19, 4]. Indexed on our vocabulary. \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "     # iterate GRU over words --> final hidden state is representation of source sentence. \n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, n_epochs, lang1, lang2, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    \"\"\"\n",
    "    pairs = load_cpickle_gc(\"train_\"+lang1.name+\"_\"+lang2.name+\"_pairs\")\n",
    "    validation_pairs = load_cpickle_gc(\"val_\"+lang1.name+\"_\"+lang2.name+\"_pairs\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train()\n",
    "            sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            outputs, encoder_hidden = encoder(sent1_batch, sent1_length_batch)\n",
    "            print(booyah)\n",
    "            break\n",
    "            \n",
    "            \n",
    "        training_pairs = [tensorsFromPair(random.choice(pairs), lang1, lang2)\n",
    "                          for i in range(n_iters)]\n",
    "        criterion = nn.NLLLoss()\n",
    "        # framing it as a categorical loss function. \n",
    "        for iter in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter - 1] \n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            \n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion, MAX_LENGTH)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n",
    "                                             iter, iter / n_epochs * 100, print_loss_avg))\n",
    "                val_loss = test_model(encoder, decoder, search, validation_pairs, lang1, max_length=MAX_LENGTH)\n",
    "                # returns bleu score\n",
    "                print(\"VALIDATION BLEU SCORE: \"+str(val_loss))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORE 8m 7s (- -9m 54s) (1000 25000%) 5.4741\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cdbb04a1ef07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-44272fa8183c>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, n_epochs, lang1, lang2, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n\u001b[1;32m     33\u001b[0m                                              iter, iter / n_epochs * 100, print_loss_avg))\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# returns bleu score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VALIDATION BLEU SCORE: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = Encoder_Batch_RNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = Decoder_RNN(target_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "num_iters = 10000\n",
    "\n",
    "args = {\n",
    "    'n_iters': 10000,\n",
    "    'n_epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'encoder': encoder1,\n",
    "    'decoder': decoder1,\n",
    "    'lang1': input_lang, \n",
    "    'lang2': target_lang,\n",
    "    'print_every': 100\n",
    "}\n",
    "\n",
    "trainIters(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder1.state_dict(), \"encoder1_40000\")\n",
    "# torch.save(decoder1.state_dict(), \"decoder1_40000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# encoder1.load_state_dict(torch.load(\"encoder1_40000\", map_location='cpu'))\n",
    "# decoder1 = DecoderRNN(target_lang.n_words, hidden_size).to(device)\n",
    "# decoder1.load_state_dict(torch.load(\"decoder1_40000\", map_location='cpu'))\n",
    "\n",
    "evaluateRandomly(encoder1, decoder1, n = 10, strategy='beam', k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10, strategy=\"greedy\", k = None):\n",
    "    \"\"\"\n",
    "    Randomly select a sentence from the input dataset and try to produce its translation.\n",
    "    \"\"\"    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = generate_translation(encoder, decoder, pair[0], search=strategy, k = k)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, search=\"greedy\", max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # encode the source lanugage\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        # output of this function\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        if search == 'greedy':\n",
    "            decoded_words = greedy_search(decoder, decoder_input, decoder_hidden, max_length)\n",
    "        elif search == 'beam':\n",
    "            decoded_words = beam_search(decoder, decoder_input, decoder_hidden, max_length)  \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "def calculate_bleu(predictions, labels):\n",
    "\t\"\"\"\n",
    "\tOnly pass a list of strings \n",
    "\t\"\"\"\n",
    "\t# tthis is ony with n_gram = 4\n",
    "\n",
    "\tbleu = sacrebleu.raw_corpus_bleu(predictions, [labels], .01).score\n",
    "\treturn bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e = EncoderRNN(5551, 256)\n",
    "model_e.load_state_dict(torch.load(\"encoder1_40000\", map_location='cpu'))\n",
    "model_e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = DecoderRNN(19344, 256)\n",
    "model_d.load_state_dict(torch.load(\"decoder1_40000\", map_location='cpu'))\n",
    "model_d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "def test_model(encoder, decoder,search, test_pairs, lang1, max_length=MAX_LENGTH):\n",
    "    # for test, you only need the lang1 words to be tokenized,\n",
    "    # lang2 words is the true labels\n",
    "    encoder_inputs = [pair[0] for pair in test_pairs]\n",
    "    true_labels = [pair[1] for pair in test_pairs]\n",
    "    translated_predictions = []\n",
    "    for i in range(len(encoder_inputs)): \n",
    "        if i% 100== 0:\n",
    "            print(i)\n",
    "        e_input = encoder_inputs[i]\n",
    "        decoded_words = generate_translation(encoder, decoder, e_input, max_length=MAX_LENGTH)\n",
    "        translated_predictions.append(\" \".join(decoded_words))\n",
    "    return calculate_bleu(translated_predictions, true_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "    \n",
    "Yikes, teh decoder isn't' preforming very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = pickle.load(open(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_indices_pairs_test\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang = pickle.load(open(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_elmo_vilang\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model_e, model_d, \"greedy\", test_pairs, input_lang )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
