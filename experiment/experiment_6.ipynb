{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: Add Attention Mask to basic attention model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader\n",
    "import argparse\n",
    "import rnn_models\n",
    "# from beam_search import *\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchtext import data\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pdb\n",
    "import sacrebleu\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import io\n",
    "import os\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--max_vocab_size'], dest='max_vocab_size', nargs=None, const=None, default=100000, type=<class 'int'>, choices=None, help='at most n tokens in vocabulary', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument(\"--max_sentence_length\", help=\"maximum sentence length\", type=int, default=50)\n",
    "parser.add_argument(\"--min_freq\", help=\"filter out tokens less than min frequency\", type=int, default=3)\n",
    "parser.add_argument(\"--max_vocab_size\", help=\"at most n tokens in vocabulary\", type=int, default=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NO = 6\n",
    "OUTPUT_FILE = 'output_{}.txt'.format(EXPERIMENT_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \n",
    "    #########\n",
    "    # Paths #\n",
    "    #########\n",
    "    \n",
    "    data = '/scratch/vr1059/vi-en/'\n",
    "    train_prefix = 'train'\n",
    "    val_prefix = 'dev'\n",
    "    test_prefix = 'test'\n",
    "    src_ext = '.tok.vi'\n",
    "    trg_ext = '.tok.en'\n",
    "\n",
    "    max_sentence_length = 50\n",
    "    min_freq = 1\n",
    "    max_vocab_size = 100000\n",
    "    \n",
    "    ################\n",
    "    # Model params #\n",
    "    ################\n",
    "    \n",
    "    hidden_size = 500\n",
    "    embedding_size = 500\n",
    "    bidirectional = True\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    attn_model = 'general'\n",
    "    lr = 5e-4\n",
    "    epochs = 12\n",
    "    batch_size = 64\n",
    "    print_every = 1000\n",
    "    clip = 0.1\n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(args, 'hidden_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common source vocabs: [(',', 128638), ('.', 120849), ('là', 51451), ('và', 47993), ('một', 40378), ('tôi', 38381), ('những', 37809), ('của', 36330), ('có', 26166), ('bạn', 26111)]\n",
      "source vocab size: 42152\n",
      "most common english vocabs: [(',', 156165), ('.', 132505), ('the', 109723), ('and', 79673), ('to', 65979), ('of', 60510), ('a', 55374), ('that', 49320), ('i', 43629), ('in', 41318)]\n",
      "english vocab size: 47859\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, src, trg = loader.load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "1268\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(phase, args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, device):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    ###########\n",
    "    # Encoder #\n",
    "    ###########\n",
    "    \n",
    "    seq_len, batch_size = batch.trg[0].shape\n",
    "    hidden = encoder.random_init_hidden(device, batch_size)\n",
    "    encoder_outputs, hidden, encoder_padding_mask = encoder(hidden, batch.src[0], batch.src[1])\n",
    "        \n",
    "    ###########\n",
    "    # Decoder #\n",
    "    ###########\n",
    "    \n",
    "    # Teacher-forcing always ON\n",
    "    \n",
    "    # [2, 2, 2, ..., 2]. List of SOS tokens, batch-sized. \n",
    "    decoder_input = batch.trg[0][0,:] \n",
    "    eos_encountered_list = [False]*batch_size\n",
    "    \n",
    "    i = 0\n",
    "    loss = 0\n",
    "    number_of_loss_calculation = 0\n",
    "    \n",
    "    # decoder.hidden = encoder.hidden[:decoder.n_layers] \n",
    "    # Use last (forward) hidden state from encoder #TODO: verify\n",
    "    hidden = hidden[:decoder.n_layers]\n",
    "    \n",
    "    while ((i+1 < seq_len) and (sum(eos_encountered_list) < batch_size)):\n",
    "        \n",
    "        logits, _, hidden = decoder(hidden, decoder_input, encoder_outputs, encoder_padding_mask)\n",
    "        logits = logits.unsqueeze(0)\n",
    "        class_probs = F.log_softmax(logits, dim = 2)\n",
    "        decoder_input = batch.trg[0][i+1,:]\n",
    "        \n",
    "        # i+1 represents the current index in all sequences\n",
    "        for j in range(batch_size):\n",
    "            if not eos_encountered_list[j]:\n",
    "                loss += loss_func(class_probs[0, j, :].view(1, -1), batch.trg[0][i+1, j].view(1))\n",
    "                number_of_loss_calculation += 1\n",
    "                \n",
    "                if batch.trg[0][i+1, j] == EOS_IDX:\n",
    "                    eos_encountered_list[j] = True\n",
    "                    \n",
    "        i += 1\n",
    "        \n",
    "     \n",
    "    # calculate gradients on each parameter\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip if too large\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), args.clip)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), args.clip)\n",
    "\n",
    "    # take gradient step\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "        \n",
    "    # report avg loss over minibatch\n",
    "    return loss.item()/number_of_loss_calculation\n",
    "\n",
    "\n",
    "#               #\n",
    "# Loss function #\n",
    "#               #\n",
    "\n",
    "# loss += loss_func(output[0, j, :].view(1, -1), batch.trg[0][i+1, j].view(1))\n",
    "                \n",
    "# so the way NLLLoss is set up, the target is simply the index that you want to predict. \n",
    "# and the input can be a softmax over the entire output vocabulary space\n",
    "# and nllloss calculate loss value between that index between predicted and \n",
    "# elementary vector e_target_idx (zeroes everywhere except 1 in target index position)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, device, epoch_idx, \n",
    "                 train_data, val_data, trg):\n",
    "    \n",
    "    \n",
    "    # Create batches with pre-sorted, similar-length sequences in each\n",
    "    train_iter = data.BucketIterator(\n",
    "        dataset=train_data, \n",
    "        batch_size=args.batch_size,\n",
    "        repeat=False,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        sort_within_batch=True,\n",
    "        device=device,\n",
    "        train=True\n",
    "    )\n",
    "\n",
    "    # Set training flag\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for i, batch in enumerate(iter(train_iter)):\n",
    "        avg_loss = train_batch('train', args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, device)\n",
    "        train_losses.append(avg_loss)\n",
    "        if args.print_every and i % args.print_every == 0:\n",
    "            print(\"train, epoch: {}, batch number: {}, batch loss: {}\".format(\n",
    "            epoch_idx, i, avg_loss))\n",
    "            with open(OUTPUT_FILE, 'a') as f:\n",
    "                f.write(\"train, epoch: {}, batch number: {}, batch loss: {}\".format(\n",
    "                    epoch_idx, i, avg_loss))\n",
    "                f.close()\n",
    "            \n",
    "            \n",
    "    print(\"epoch: {}, average loss for epoch: {}, size of last batch {}\".format(\n",
    "    epoch_idx, np.mean(train_losses), batch.src[0].shape[1]))\n",
    "    with open(OUTPUT_FILE, 'a') as f:\n",
    "        f.write(\"epoch: {}, average loss for epoch: {}, size of last batch {}\".format(\n",
    "            epoch_idx, np.mean(train_losses), batch.src[0].shape[1]))\n",
    "        f.close()\n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(predictions, labels):\n",
    "    \"\"\"\n",
    "    Only pass a list of strings \n",
    "    \"\"\"\n",
    "    # n_gram = 4\n",
    "\n",
    "    bleu = sacrebleu.raw_corpus_bleu(predictions, [labels], .01).score\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00000000000004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(['I am rich. '], ['I am rich.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(decoder, decoder_input, encoder_outputs, hidden, max_length, k, trg, mask = None):\n",
    "    \n",
    "    candidates = [(decoder_input, 0, hidden)]\n",
    "    potential_candidates = []\n",
    "    completed_translations = []\n",
    "\n",
    "    # put a cap on sentence length\n",
    "    for m in range(max_length):\n",
    "        for c in candidates:\n",
    "            # unpack the tuple\n",
    "            c_sequence, c_score, c_hidden = c\n",
    "            \n",
    "            # EOS token\n",
    "            if c_sequence[-1] == EOS_IDX:\n",
    "                completed_translations.append((c_sequence, c_score))\n",
    "                k = k - 1\n",
    "            else:\n",
    "                logits, _, hidden = decoder(c_hidden.contiguous()[:decoder.n_layers], c_sequence.contiguous()[-1].unsqueeze(0), encoder_outputs, mask)\n",
    "                next_word_probs = F.log_softmax(logits, dim = 1)\n",
    "                # in the worst-case, one sequence will have the highest k probabilities\n",
    "                # so to save computation, only grab the k highest_probability from each candidate sequence\n",
    "                top_probs, top_idx = torch.topk(next_word_probs, k)\n",
    "                top_probs.squeeze_()\n",
    "                top_idx.squeeze_()\n",
    "                top_probs = [top_probs] if len(top_probs.size()) == 0 else top_probs\n",
    "                top_idx = [top_idx] if len(top_idx.size()) == 0 else top_idx\n",
    "                for i in range(len(top_probs)):\n",
    "                    word = top_idx[i].reshape(1, 1).to(device)\n",
    "                    new_score = c_score + top_probs[i]\n",
    "                    potential_candidates.append((torch.cat((c_sequence, word)).to(device), new_score, hidden))\n",
    "\n",
    "        candidates = sorted(potential_candidates, key= lambda x: x[1], reverse=True)[0:k] \n",
    "        potential_candidates = []\n",
    "\n",
    "    completed = completed_translations + candidates\n",
    "    completed = sorted(completed, key= lambda x: x[1], reverse=True)[0] \n",
    "    return completed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_words(ids, trg):\n",
    "    words = \"\"\n",
    "    for x in ids:\n",
    "        words += trg.vocab.itos[x.squeeze().item()] + ' '\n",
    "    return words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaitra said the best way to get hyperparameters for your model\n",
    "# was to take like 100, 200, 500 examples from your dataset\n",
    "# and see which hyperparameter combination overfits the best/fastest on that example. \n",
    "# And then use that as your hyperparameter combination to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_batch(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, trg, device):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    ############\n",
    "    #  encode  #\n",
    "    ############\n",
    "    \n",
    "    _, batch_size = batch.trg[0].shape\n",
    "    hidden = encoder.random_init_hidden(device, batch_size)\n",
    "    encoder_outputs, hidden, _ = encoder(hidden, batch.src[0], batch.src[1])\n",
    "    \n",
    "    #################\n",
    "    #  beam search  #\n",
    "    #################\n",
    "    \n",
    "    max_length = 30\n",
    "    k = 2 \n",
    "    \n",
    "    translations = []\n",
    "    trg_translations = []\n",
    "    for i in range(batch_size):\n",
    "        decoder_input = torch.tensor([[src.vocab.stoi['SOS']]], device=device)\n",
    "        decoder_hidden = hidden[:, i, :].unsqueeze(1)\n",
    "        encoder_outputs_i = encoder_outputs[:, i, :].unsqueeze(1)\n",
    "\n",
    "        seq_of_ids = beam_search(decoder, decoder_input, encoder_outputs_i, decoder_hidden, max_length, k, trg)\n",
    "        translations.append(ids_to_words(seq_of_ids, trg))\n",
    "        trg_translations.append(ids_to_words(batch.trg[0][:batch.trg[1][i], i], trg))\n",
    "        \n",
    "    return translations, trg_translations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, device, epoch_idx, \n",
    "        val_data, trg):\n",
    "    \n",
    "    # Create minibatches over validation data\n",
    "    val_iter = data.BucketIterator(\n",
    "        dataset=val_data, \n",
    "        batch_size=args.batch_size,\n",
    "        train=False,\n",
    "        shuffle=False,\n",
    "        # A key to use for sorting examples in order to batch together \n",
    "        # examples with similar lengths and minimize padding.\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        repeat=False,\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_losses = []\n",
    "    val_bleus = []\n",
    "    val_references = []\n",
    "    \n",
    "    all_predicted = []\n",
    "    all_trg = []\n",
    "    for i, batch in enumerate(iter(val_iter)):\n",
    "        predicted_trans, trg_trans = val_batch(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, trg, device)\n",
    "        all_predicted += predicted_trans\n",
    "        all_trg += trg_trans\n",
    "    print(all_predicted[:10])\n",
    "    print(all_trg[:10])    \n",
    "    bleu = calculate_bleu(all_predicted, all_trg)\n",
    "    print(bleu)\n",
    "    if epoch_idx % 3 == 0:\n",
    "        with open(OUTPUT_FILE, 'a') as f:\n",
    "            f.write(str(all_predicted[:10]))\n",
    "            f.write(str(all_trg[:10]))\n",
    "            f.write(\"bleu score: {}\".format(bleu))\n",
    "            f.close()\n",
    "    return bleu\n",
    "    \n",
    "    \n",
    "#                    #\n",
    "# Batch & Dimensions #\n",
    "#                    #\n",
    "# `batch` represents a batch of examples. \n",
    "# `batch.src` consists of two tensors. \n",
    "# The first, `b.src[0]`, is the `src` examples from your batch; it's a tensor with the shape (max_seq_len, batch_size). \n",
    "# Your sequences have already been indexed and padded. \n",
    "# The second, `b.src[1]`, is the actual lengths of each sequence. It is of shape (batch_size, 1). \n",
    "\n",
    "# data.BucketIterator automatically batches sequences of similar lengths together. \n",
    "# it also automatically sorts in reverse order. \n",
    "\n",
    "# Say you have a bidirectional, 2-layer RNN encoder. A single batch has max length 19 and batch size 32. \n",
    "# The encoder_outputs will have shape: (19, 32, 512). \n",
    "# Basically, it only returns the topmost layer's hidden states at each step of the sequence. \n",
    "# And it concatenates both directional outputs (hidden states) for the topmost layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, epoch: 0, batch number: 0, batch loss: 10.776028394358011\n",
      "train, epoch: 0, batch number: 1000, batch loss: 4.511085283401799\n",
      "train, epoch: 0, batch number: 2000, batch loss: 4.911072368642357\n",
      "epoch: 0, average loss for epoch: 5.237080740595414, size of last batch 64\n",
      "[\"SOS you can 't be a lot . EOS\", \"SOS and it 's not so much . EOS\", \"SOS it 's a lot of wheelbarrow . EOS\", \"SOS there 's a lot of two years . EOS\", 'SOS this is a year . EOS', \"SOS it 's going to be able . EOS\", 'SOS this is a homemaker . EOS', \"SOS we 've been overactive . EOS\", 'SOS do does the world ? EOS', \"SOS so i 've been beaten . EOS\"]\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "3.004621088811569\n",
      "train, epoch: 1, batch number: 0, batch loss: 4.678447936225858\n",
      "train, epoch: 1, batch number: 1000, batch loss: 3.5234038781298906\n",
      "train, epoch: 1, batch number: 2000, batch loss: 4.257747062954118\n",
      "epoch: 1, average loss for epoch: 4.123741752590551, size of last batch 64\n",
      "['SOS you have a lot of things . EOS', \"SOS the question is that it 's not . EOS\", \"SOS it 's diddly-point-squat . EOS\", \"SOS there 's diddly-point-squat . EOS\", 'SOS this is vivienne arabia . EOS', \"SOS it 's evolving . EOS\", 'SOS this is the beginning . EOS', \"SOS so we 've got rid of the atmosphere . EOS\", 'SOS look at this ? EOS', 'SOS so i went into . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "4.9817809235903985\n",
      "train, epoch: 2, batch number: 0, batch loss: 4.021185049512697\n",
      "train, epoch: 2, batch number: 1000, batch loss: 3.048934912644268\n",
      "train, epoch: 2, batch number: 2000, batch loss: 3.8475214339175188\n",
      "epoch: 2, average loss for epoch: 3.612962810999006, size of last batch 64\n",
      "[\"SOS you 've got a lot of complexity . EOS\", 'SOS my answer is the answer . EOS', \"SOS it 's diddly-point-squat . EOS\", 'SOS in spain . EOS', 'SOS this is vivienne arabia . EOS', \"SOS it 's been celebrated . EOS\", \"SOS here 's the first example . EOS\", \"SOS we 've analyzed the internet . EOS\", 'SOS think about this ? EOS', \"SOS i 've been celebrated . EOS\"]\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "6.5018813070313755\n",
      "train, epoch: 3, batch number: 0, batch loss: 3.603035027718169\n",
      "train, epoch: 3, batch number: 1000, batch loss: 2.650961241625098\n",
      "train, epoch: 3, batch number: 2000, batch loss: 3.5630476739101185\n",
      "epoch: 3, average loss for epoch: 3.2332438305377087, size of last batch 64\n",
      "[\"SOS you 've got a perfect machine . EOS\", 'SOS my mother was curious . EOS', 'SOS shame is vivienne disenfranchisement . EOS', 'SOS in sub-zero paulo . EOS', 'SOS this is evangelina firestone . EOS', \"SOS it 's been celebrated . EOS\", 'SOS this is the first divergence . EOS', 'SOS so we measured them into the community . EOS', 'SOS think about this ? EOS', 'SOS i was celebrated . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "7.263378685287384\n",
      "train, epoch: 4, batch number: 0, batch loss: 3.3070384449883763\n",
      "train, epoch: 4, batch number: 1000, batch loss: 2.328049541825606\n",
      "train, epoch: 4, batch number: 2000, batch loss: 3.3352174296959096\n",
      "epoch: 4, average loss for epoch: 2.9259489063475557, size of last batch 64\n",
      "['SOS you have a perfect machine . EOS', 'SOS my mother was curious . EOS', 'SOS md is the limits of course . EOS', 'SOS in svalbard . EOS', 'SOS this is evangelina firestone . EOS', \"SOS it 's a lot closer . EOS\", 'SOS this is the first prototype . EOS', 'SOS we measured the skills . EOS', 'SOS think about it . EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "8.330454548808827\n",
      "train, epoch: 5, batch number: 0, batch loss: 3.046321405020565\n",
      "train, epoch: 5, batch number: 1000, batch loss: 2.0361920328393976\n",
      "train, epoch: 5, batch number: 2000, batch loss: 3.1016460702031754\n",
      "epoch: 5, average loss for epoch: 2.6627779235890783, size of last batch 64\n",
      "['SOS you have a perfect memory . EOS', 'SOS my mother was curious . EOS', 'SOS md is the key . EOS', 'SOS in svalbard , in svalbard . EOS', 'SOS this is evangelina firestone . EOS', \"SOS it 's been bathed . EOS\", 'SOS this is the first prototype . EOS', 'SOS we measured the skills around us . EOS', 'SOS think of that ? EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "8.521124917183704\n",
      "train, epoch: 6, batch number: 0, batch loss: 2.8719581964189915\n",
      "train, epoch: 6, batch number: 1000, batch loss: 1.769815507629108\n",
      "train, epoch: 6, batch number: 2000, batch loss: 2.89887348417317\n",
      "epoch: 6, average loss for epoch: 2.4345266933467435, size of last batch 64\n",
      "['SOS you have a perfect memory . EOS', 'SOS my mother was really proud of me . EOS', 'SOS morgana is a symbol of faith . EOS', 'SOS in the winter , floating down . EOS', 'SOS this is the foyer . EOS', \"SOS it 's been bathed . EOS\", 'SOS this is the first prototype . EOS', 'SOS we measured the skills around . EOS', 'SOS think about that ? EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "8.904343269061311\n",
      "train, epoch: 7, batch number: 0, batch loss: 2.6406269209920423\n",
      "train, epoch: 7, batch number: 1000, batch loss: 1.534031159068124\n",
      "train, epoch: 7, batch number: 2000, batch loss: 2.7364897466193487\n",
      "epoch: 7, average loss for epoch: 2.2367878776765733, size of last batch 64\n",
      "['SOS you have a perfect memory . EOS', 'SOS my name was innocent . EOS', 'SOS liu bolin is a symbol of authenticity . EOS', 'SOS at the bottom , the height is cloudy . EOS', 'SOS this is the foyer room . EOS', \"SOS it 's been bathed . EOS\", 'SOS this is the first prototype . EOS', 'SOS we were selectively engaged . EOS', 'SOS think of that ? EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.435334146267284\n",
      "train, epoch: 8, batch number: 0, batch loss: 2.420302755331277\n",
      "train, epoch: 8, batch number: 1000, batch loss: 1.3271276148645343\n",
      "train, epoch: 8, batch number: 2000, batch loss: 2.5855133417989773\n",
      "epoch: 8, average loss for epoch: 2.061616071305829, size of last batch 64\n",
      "['SOS you have a perfect memory . EOS', 'SOS sunitha knew what the love is . EOS', 'SOS politeness is a symbol of authenticity . EOS', \"SOS in the dark corner , it 's full of mountains . EOS\", 'SOS this is the patkotak mites community . EOS', \"SOS it 's been bathed . EOS\", 'SOS this is the first prototype . EOS', 'SOS we selectively communicate communities . EOS', 'SOS think about that ? EOS', 'SOS and i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "9.21871233889023\n",
      "train, epoch: 9, batch number: 0, batch loss: 2.258913926982743\n",
      "train, epoch: 9, batch number: 1000, batch loss: 1.168355019439554\n",
      "train, epoch: 9, batch number: 2000, batch loss: 2.4471959831640206\n",
      "epoch: 9, average loss for epoch: 1.9080749889521689, size of last batch 64\n",
      "['SOS you have a perfect memory . EOS', 'SOS king knew the love to know . EOS', \"SOS ralph moore 's faith EOS\", 'SOS at the height of the rock , the tentacles EOS', 'SOS this is the kitchen man . EOS', \"SOS it 's a time lapse . EOS\", 'SOS this is the first prototype . EOS', 'SOS we selectively communicate . EOS', 'SOS think of that ? EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "9.123732496068941\n",
      "train, epoch: 10, batch number: 0, batch loss: 2.195214005498927\n",
      "train, epoch: 10, batch number: 1000, batch loss: 1.026404916587197\n",
      "train, epoch: 10, batch number: 2000, batch loss: 2.306080599518972\n",
      "epoch: 10, average loss for epoch: 1.7674128192122533, size of last batch 64\n",
      "['SOS you have a complete memory . EOS', 'SOS sunitha knew the love . EOS', 'SOS morgana williams de botton . EOS', 'SOS in front of the dark side , the spurious refined . EOS', 'SOS this is the kitchen . EOS', \"SOS it 's been towed . EOS\", 'SOS this is the first mystery . EOS', 'SOS we selectively communicate . EOS', 'SOS think of that ? EOS', 'SOS i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "9.316761448560657\n",
      "train, epoch: 11, batch number: 0, batch loss: 2.027416747697604\n",
      "train, epoch: 11, batch number: 1000, batch loss: 0.8849794868385661\n",
      "train, epoch: 11, batch number: 2000, batch loss: 2.1620830950618943\n",
      "epoch: 11, average loss for epoch: 1.6408321041461924, size of last batch 64\n",
      "[\"SOS you 've got a perfect memory . EOS\", 'SOS harriet knew that . EOS', 'SOS ms. madness is a possibility of paralysis . EOS', \"SOS in front of greenland glands , the night 's side . EOS\", 'SOS this is the hospital worker . EOS', \"SOS it 's a time . EOS\", 'SOS this is the first sight . EOS', 'SOS we tweak the communities . EOS', 'SOS think about that ? EOS', 'SOS and i was thrilled . EOS']\n",
      "['SOS he has the most <unk> memory . EOS', 'SOS remi knows what love is . EOS', 'SOS kofi is the embodiment of possibility . EOS', 'SOS in the cold , windy night . EOS', 'SOS this is a family portrait . EOS', 'SOS it was very time-consuming . EOS', 'SOS this was the very first . EOS', 'SOS we activate communities . EOS', 'SOS guess what ? EOS', 'SOS and i was distraught . EOS']\n",
      "9.349690804017099\n"
     ]
    }
   ],
   "source": [
    "src_padding_idx = src.vocab.stoi['<pad>']\n",
    "trg_padding_idx = trg.vocab.stoi['<pad>']\n",
    "EOS_IDX = trg.vocab.stoi['EOS']\n",
    "\n",
    "encoder = rnn_models.Encoder(args, src_padding_idx, len(src.vocab)).to(device)\n",
    "decoder = rnn_models.LuongAttnDecoderRNN(args, trg_padding_idx, len(trg.vocab)).to(device)\n",
    "\n",
    "# initialize weights using gaussian with 0 mean and 0.01 std, just like the paper said\n",
    "# TODO: Better initialization. Xavier?\n",
    "for net in [encoder, decoder]:\n",
    "    for name, param in net.named_parameters(): \n",
    "        #print(name, type(param), param)\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0.0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.xavier_normal_(param)\n",
    "            \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=args.lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=args.lr)\n",
    "enc_scheduler = ReduceLROnPlateau(encoder_optimizer, min_lr=1e-10,factor = 0.5,  patience=0)\n",
    "dec_scheduler = ReduceLROnPlateau(decoder_optimizer, min_lr=1e-10,factor = 0.5,  patience=0)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "loss_history = []\n",
    "bleu_history = []\n",
    "\n",
    "for i in range(args.epochs):\n",
    "    train_loss = train(args, encoder, decoder, encoder_optimizer, \n",
    "                                     decoder_optimizer, loss_func, device, i, \n",
    "                                    train_data, val_data, trg)\n",
    "    \n",
    "    loss_history.append(train_loss)\n",
    "    bleu = val(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, device, i, val_data, trg)\n",
    "    bleu_history.append(bleu)\n",
    "    \n",
    "    torch.save(encoder.state_dict(), 'vi-en_encoder_exp_{}.pth'.format(EXPERIMENT_NO))\n",
    "    torch.save(decoder.state_dict(), 'vi-en_decoder_exp_{}.pth'.format(EXPERIMENT_NO))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = [\"i am a foon\", \"boondawg in the moon.\", \"Yippe.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "func()\n",
    "with open('output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        time.sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
