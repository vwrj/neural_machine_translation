{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader\n",
    "import argparse\n",
    "import rnn_models\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import io\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--max_vocab_size'], dest='max_vocab_size', nargs=None, const=None, default=100000, type=<class 'int'>, choices=None, help='at most n tokens in vocabulary', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Testing')\n",
    "parser.add_argument(\"--max_sentence_length\", help=\"maximum sentence length\", type=int, default=50)\n",
    "parser.add_argument(\"--min_freq\", help=\"filter out tokens less than min frequency\", type=int, default=3)\n",
    "parser.add_argument(\"--max_vocab_size\", help=\"at most n tokens in vocabulary\", type=int, default=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \n",
    "    max_sentence_length = 50\n",
    "    min_freq = 3\n",
    "    max_vocab_size = 100000\n",
    "    data = 'data'\n",
    "    hidden_size = 256\n",
    "    embedding_size = 256\n",
    "    bidirectional = True\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    attn_model = 'default'\n",
    "    lr = 1e-3\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    \n",
    "args = Args()\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common source vocabs: [(',', 128638), ('.', 120849), ('là', 51451), ('và', 47993), ('một', 40378), ('tôi', 38381), ('những', 37809), ('của', 36330), ('có', 26166), ('bạn', 26111)]\n",
      "source vocab size: 20125\n",
      "most common english vocabs: [(',', 156165), ('.', 132505), ('the', 109723), ('and', 79673), ('to', 65979), ('of', 60510), ('a', 55374), ('that', 49320), ('i', 43629), ('in', 41318)]\n",
      "english vocab size: 22443\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, src, trg = loader.load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_padding_idx = src.vocab.stoi['<pad>']\n",
    "trg_padding_idx = trg.vocab.stoi['<pad>']\n",
    "\n",
    "encoder = rnn_models.Encoder(args, src_padding_idx, len(src.vocab))\n",
    "decoder = rnn_models.LuongAttnDecoderRNN(args, trg_padding_idx, len(trg.vocab))\n",
    "\n",
    "# initialize weights using gaussian with 0 mean and 0.01 std, just like the paper said\n",
    "# TODO: Better initialization. Xavier?\n",
    "for net in [encoder, decoder]:\n",
    "    for name, param in net.named_parameters(): \n",
    "        #print(name, type(param), param)\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0.0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.xavier_normal_(param)\n",
    "            \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=args.lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=args.lr)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "loss_history = defaultdict(list)\n",
    "bleu_history = defaultdict(list)\n",
    "\n",
    "# for i in range(args.epochs):\n",
    "#     train_loss, val_loss, val_bleu = train_and_val(args, encoder, decoder, encoder_optimizer, \n",
    "#                                                    decoder_optimizer, loss_function, device, i, \n",
    "#                                                    train_data, val_data, trg, encoder_embedding_dict, \n",
    "#                                                    decoder_embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "h = torch.randn(1, 32, 256)\n",
    "e = torch.randn(17, 32, 256)\n",
    "energy = torch.bmm(e.transpose(1, 0), h.squeeze(0).unsqueeze(2))\n",
    "# energy.shape = Size([32, 17, 1])\n",
    "score = F.softmax(energy, dim = 1).view(1, 32, -1)\n",
    "context_vector = torch.bmm(score.transpose(1, 0), e.transpose(1, 0))\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.squeeze(0).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(phase, args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, device):\n",
    "    if phase == \"train\":\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # TODO: it seems that currently batch size is always the same. Make sure to use the last batch\n",
    "    max_trg_seq_len, batch_size = batch.trg[0].shape\n",
    "    \n",
    "    hidden = encoder.random_init_hidden(device, batch_size)\n",
    "    encoder_outputs, hidden = encoder(hidden, batch.src[0], batch.src[1])\n",
    "    \n",
    "    \n",
    "    ### Decoder\n",
    "    ### Teacher-forcing\n",
    "    \n",
    "    translated_tokens_list = []\n",
    "    decoder_input = batch.trg[0][0,:] # [2, 2, 2, ..., 2]. List of SOS tokens, batch-sized. \n",
    "    translated_tokens_list.append(decoder_input.unsqueeze(0))\n",
    "    eos_encountered_list = [False]*batch_size\n",
    "    i = 0\n",
    "    \n",
    "    while ((i+1 < target_sequence_length)  and (sum(eos_encountered_list) < batch_size)):\n",
    "        logits, decoder_attn, hidden, cell_state = decoder(\n",
    "                hidden, cell_state, decoder_input, encoder_outputs, src_lengths=batch.src[1]\n",
    "            )\n",
    "    \n",
    "    \n",
    "    return 5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = data.BucketIterator(\n",
    "        dataset=train_data, \n",
    "        batch_size=args.batch_size,\n",
    "        repeat=False,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        sort_within_batch=True,\n",
    "        device=device,\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "val_iter = data.BucketIterator(\n",
    "    dataset=val_data, \n",
    "    batch_size=args.batch_size,\n",
    "    train=False,\n",
    "    shuffle=False,\n",
    "    #A key to use for sorting examples in order to batch together \n",
    "    # examples with similar lengths and minimize padding.\n",
    "    sort=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    repeat=False,\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "train_losses = []\n",
    "batch = next(iter(train_iter))\n",
    "batch.trg[0][0,:].shape\n",
    "# for i in range(np.random.randint(20)):\n",
    "    \n",
    "#for i, batch in enumerate(iter(train_iter)):\n",
    "\n",
    "# loss = run_batch('train', args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, batch, device)\n",
    "    \n",
    "\n",
    "#                    #\n",
    "# Batch & Dimensions #\n",
    "#                    #\n",
    "# `batch` represents a batch of examples. \n",
    "# `batch.src` consists of two tensors. \n",
    "# The first, `b.src[0]`, is the `src` examples from your batch; it's a tensor with the shape (max_seq_len, batch_size). \n",
    "# Your sequences have already been indexed and padded. \n",
    "# The second, `b.src[1]`, is the actual lengths of each sequence. It is of shape (batch_size, 1). \n",
    "\n",
    "# data.BucketIterator automatically batches sequences of similar lengths together. \n",
    "# it also automatically sorts in reverse order. \n",
    "\n",
    "# Say you have a bidirectional, 2-layer RNN encoder. A single batch has max length 19 and batch size 32. \n",
    "# The encoder_outputs will have shape: (19, 32, 512). \n",
    "# Basically, it only returns the topmost layer's hidden states at each step of the sequence. \n",
    "# And it concatenates both directional outputs (hidden states) for the topmost layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.trg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_val(args, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_function, device, i, \n",
    "#                   train_data, val_data, trg, encoder_embedding_dict, decoder_embedding_dict):\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
