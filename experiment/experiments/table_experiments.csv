id~question~outcome
1~can i overfit on a small dataset~yes but i have to crank up the epochs to a lot and loss values were very noisy at first which frustrated me but it seems to learn slowly at first and at epoch 49 you can clearly see that it memorizes the train data and train loss is very low (like 1.97)
2~how well can i do using full train set & standard attention (lr scheduler) (no hyperparam search) (no masking) (clip 1)~pretty bad 5e-3 Adam & gradient clip 1 never really learned hm it got from 10.7 to 7 and then oscillated around 6 and 7 for 11 epochs hmm i wonder if my modules are working properly or maybe beam search maybe i should try just greedy search
3~losses in 2 don't look good (seem to be oscillating around same values every epoch) so will decreasing the learning rate and gradient clip to 0.1 work~absolutely great: all i had to do was decrease learning rate from 5e-3 to 5e-4 and suddenly loss values go all the way down until 1.7 & bleu score is 11 & the model is very clearly working wow...along with learning rate and lr scheduler my beam search seems to be working perfectly!
4~does adding pre-trained FastText word vectors improve model performance? Here model is the standard attention model above (no masking) (no real hyperparam search) using 5e-4 Adam and 0.1 gradient clip ~ Nope, after 11 epochs, the bleu score is 6 instead of 11. Hm maybe it's the learning rate or some hyperparameter search? I wonder if doing a hyperparameter search on a very small subset of the data is the right way to go? 
5~Maybe training with FastText vectors requires a bigger learning rate? Let's try lr of 1e-3 instead of 5e-4..~So I tried three different things: increasing lr, decreasing lr, and increasing gradient clip to 0.5 from 0.1. No change. Using FastText vectors on basic attention model doesn't seem to help? That's weird. 
6~Try the Attention Mask on basic attention model (Adam 5e-4, clip 0.1) ~ Honestly doesn't seem to improve convergence. Bleu score after 11 epochs is like 9ish, 10ish. 
7~I'm using a beam search window of 2... What happens if I increase it to 10?~Great! Bleu score improved to 18.37 without changing any of the other hyperparams. Highest bleu score yet. 
8~How is Chaitra calculating bleu score? I want to try stripping away SOS and EOS tokens at the beginning, because I think those can artificially boost the BLEU score. ~ Yeah BLEU score ended up being 16.3 instead of 18.47. Interesting. Although maybe I trained it too much, because the 18 BLEU score doesn't correspond to the one with the highest val bleu score.  
9~So...I just found out that I'm not using the scheduler! LOL let me try to use it (scheduler.step()) instead of (optimizer.step()). I'm doing it based on val BLEU score instead of val loss because it's easier to calculate and BLEU is a good proxy for val loss anyways~Okay so three runs. The third run got me 18 BLEU score with the new BLEU score calculation (clipping off SOS and EOS)! Good stuff. The second run kept oscillating around 7 BLEU and I was like what is going on. I guess it was just a bad run (bad random initialization)
10~I'd like to implement FP16 training and see if it's faster...~outcome
11~I wonder if the dot product Attention method leads to better results...~outcome
